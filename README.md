# MusicAnalysis - A Machine Learning Project

## Introduction

Welcome to MusicAnalysis, a Machine Learning project undertaken by Edward Lee and John Lee. In this project, we explored various iterations, faced challenges, utilized various technologies, and finally, developed a capable music analysis model.

## Iterations

## Pre-Iteration: Tutorial

Our model was initially based on Valerio Velardo's Deep Learning (for audio) tutorial, which allowed the user to categorize songs based on genre. Using the Spotify API to gain metadata on various tracks, along with training our model with different groups of music, we were able to create a unique model.

### Iteration 1: Data Collection and Preprocessing

In the first iteration, we focused on using two playlists (one happy, one sad) to see if we could make it distinguish between happy and sad songs using the MFCC data that we received from the songs. One challenge we ran into initially was having a very high accuracy on the training data with very poor actual test results. Doing more research and creating visualized training/test graphs to figure out the issue, we identified hte issue to be overfitting. After improving upon the model by adding Dropout layers and optimizations, we were able to get the model to have about a 62% accuracy rate. To make a more precise model, we decided to increase the parameters for songs from just MFCC to include energy, tempo, etc. using Spotify provided data.

### Iteration 2: Further Data Collection

In this step, we focused on understanding the model in order to have it factor in other elements. The other data collected using the Spotify API included energy, danceability, loudness, tempo, and valence. By first creating a Spotify.py file that collected 30 second clips of each song from Spotify, along with a label stating happy or sad, the model was able to fairly confidently distinguish between the two- according to the test. Something that we really struggled with was creating our own tester file, as it would have to create a JSON file containing the same data/ format as the model, and compare that to the model.h5 generated by the CNN_Classifier.

### Iteration 3: Successful Happy/Sad Model

After various failed attempts, a successful test was made, allowing for us to check the accuracy of the model with real samples. As expected, the accuracy of the model in determining happy and songs was much higher improving to a rate of 94. Now that it was also accounting for 5 other factors that indicate the tone of a song, the model was working much better. The only small bug was a misordering of the functions, wherein the comparison of the test playlist and the model occurred before the JSON file was created. Going forward, we are hoping to expand on the functionality of the model.

## Challenges Faced

1. **Data Collection:** Due to copyright restrictions and file conversion issues, there was an initial concern as to where we'd get data, but further inspection into Spotify's API gave us 30 second clips for as many songs as we needed.

## Technologies Used

- Python: The primary programming language for the project.
- TensorFlow: Deep learning frameworks used for model development and training.
- Librosa: A Python library for audio and music analysis, used for feature extraction.
- Spotify API

## Final Product Capabilities

The final product of MusicAnalysis is a user-friendly web application capable of real-time music analysis. Its key features include:

1. **Genre Classification:** TBD
